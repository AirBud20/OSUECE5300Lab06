{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab:  Nonlinear Least Squares for Modeling Materials\n",
    "\n",
    "In this lab, we will explore gradient descent on nonlinear least squares.  \n",
    "\n",
    "Suppose we wish to fit a model of the form,\n",
    "\n",
    "     yhat ~= f(x,w)\n",
    "     \n",
    "where `x` is a vector of features, `w` is a vector of parameters and `f` is a nonlinear function of `w`.  Often we find the parameters `w` that minimize a squared-error cost of the form \n",
    "\n",
    "     J(w) = \\sum_i (y_i - f(x_i,w))^2\n",
    "     \n",
    "where the summation is over training samples `(x_i,y_i)`.  This problem is known as nonlinear least-squares (NLLS).  In general, this optimization problem has no closed-form expression, and so gradient descent is widely used.  \n",
    "\n",
    "In this lab, we will apply NLLS to the physical modeling of materials.  Specifically, we will estimate parameters in a model for the expansion of copper as a function of temperature.  In doing this lab, you will learn to:\n",
    "* Set up a nonlinear least squares as an unconstrained optimization function\n",
    "* Compute initial parameter estimates for a simple rational model\n",
    "* Compute the gradients of the least squares objective\n",
    "* Implement gradient descent for minimizing the objective\n",
    "* Implement momentum gradient descent\n",
    "* Visualize the convergence of the algorithm\n",
    "\n",
    "We first import some key packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "The NIST agency has an excellent [nonlinear regression website](https://www.itl.nist.gov/div898/strd/nls/nls_main.shtml) that has several datasets appropriate for nonlinear regression problems.  In this lab, we will use the data from a NIST study involving the thermal expansion of copper. The response variable is the coefficient of thermal expansion, and the predictor variable is temperature in degrees kelvin.  \n",
    "\n",
    "> Hahn, T., NIST (1979), Copper Thermal Expansion Study.  (unpublished}\n",
    "\n",
    "You can download the data as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.591</td>\n",
       "      <td>24.41</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.547</td>\n",
       "      <td>34.82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.902</td>\n",
       "      <td>44.09</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.894</td>\n",
       "      <td>45.07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.703</td>\n",
       "      <td>54.98</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x0     y0  dummy\n",
       "0  0.591  24.41    NaN\n",
       "1  1.547  34.82    NaN\n",
       "2  2.902  44.09    NaN\n",
       "3  2.894  45.07    NaN\n",
       "4  4.703  54.98    NaN"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://itl.nist.gov/div898/strd/nls/data/LINKS/DATA/Hahn1.dat'\n",
    "df = pd.read_csv(url, skiprows=60, sep=' ',skipinitialspace=True, names=['x0','y0','dummy'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the `x0` and `y0` into arrays.  Rescale `x0` and `y0` to values between `0` and `1` by dividing `x0` and `y0` by the maximum value.  Store the scaled values in vectors `x` and `y`.  The rescaling will help with the conditioning of the fitting.  Plot `y` vs. `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a20110a50>]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "# x0 = ...\n",
    "# y0 = ...\n",
    "\n",
    "# plt.plot(...)\n",
    "x0 = df['x0']\n",
    "y0 = df['y0']\n",
    "x = x0/np.max(x0)\n",
    "y = y0/np.max(y0)\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Thermal Expansion')\n",
    "plt.grid(True)\n",
    "plt.plot(x,y, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the problem a little more challenging, we will add some noise.  Add random Gaussian noise with mean 0 and std. dev = 0.05 to `y`.  Store the noisy results in `yn`. You can use the `np.random.normal()` function to add Gaussian noise. Plot `yn` vs. `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# yn = y + ...\n",
    "error = np.random.normal(0, .05, y.size) \n",
    "yn = y + error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data `(x,yn)` into training and test.  Let `xtr,ytr` be training data and `xts,yts` be the test data.  You can use the `train_test_split` function.  Set `test_size=0.33` so that 1/3 of the samples are held out for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29 28 27 26  1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO\n",
    "# xtr, xts, ytr, yts = ...\n",
    "xtr, xts, ytr, yts = train_test_split(x,yn,test_size = .33)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Fit for a Rational Model\n",
    "\n",
    "The [NIST website](https://www.itl.nist.gov/div898/strd/nls/data/hahn1.shtml) suggests using a *rational* model of the form,\n",
    "\n",
    "      yhat = (a[0] + a[1]*x + ... + a[d]*x^d)/(1 + b[0]*x + ... + b[d-1]*x^d)\n",
    "      \n",
    "with `d=3`.  The model parameters are `w = [a[0],...,a[d],b[0],...,b[d-1]]`, so there are `2d+1` parameters total.    Complete the function below that takes vectors `w` and `x` and predicts a set of values `yhat` using the above model.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,x):\n",
    "    \n",
    "    # Get the length\n",
    "    d = (len(w)-1)//2\n",
    "    \n",
    "    # TODO.  Extract a and b from w\n",
    "    # a = ...\n",
    "    # b = ...\n",
    "    a = w[0:d+1]\n",
    "    b = w[d+1:2*d+2]\n",
    "    a = a[::-1]\n",
    "    b = b[::-1]\n",
    "    b = np.append(b, 1)\n",
    "    num = np.polyval(a,x)\n",
    "    dnum = np.polyval(b,x)\n",
    "    yhat = num/dnum\n",
    "    # TODO.  Compute yhat.  You may use the np.polyval function\n",
    "    # But, remember you must flip the order the a and b\n",
    "    # yhat = ...\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fit with a nonlinear model, most methods only obtain convergence to a local minimum.  For that local minimum to be the global minimum, we need a good initial condition.  For a rational model, one way to get a good initialization is to realize that if\n",
    "\n",
    "    y ~= (a[0] + a[1]*x + ... + a[d]*x^d)/(1 + b[0]*x + ... + b[d-1]*x^d)\n",
    "    \n",
    "then\n",
    "\n",
    "    y ~= a[0] + a[1]*x + ... + a[d]*x^d - b[0]*x*y + ... - b[d-1]*x^d*y.\n",
    "    \n",
    "So, we can initially solve for the parameters `w = [a,b]` using linear regression with the features\n",
    "\n",
    "    Z[i,:] = [ x[i], ... , x[i]**d, y[i]*x[i], ... , y[i}*x[i]**d ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00 -4.82835672e-17 -3.82101058e-17 -4.82835672e-17\n",
      " -3.82101058e-17  4.64677638e-17]\n",
      "2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "d = 3\n",
    "\n",
    "# TODO.  Create the transformed feature matrix\n",
    "# Z = ...\n",
    "xtr = np.array(xtr)\n",
    "ytr = np.array(xtr)\n",
    "Z = np.zeros((len(xtr), 2*d))\n",
    "for j in range(d):\n",
    "    for i in range(len(xtr)):\n",
    "        Z[i, j] = xtr[i]**(j+1)\n",
    "\n",
    "for j in range(d):\n",
    "    for i in range(len(xtr)):\n",
    "        Z[i, j+3] = xtr[i]*ytr[i]**(j+1)\n",
    "\n",
    "       \n",
    "    \n",
    "# TODO.  Fit with parameters with linear regression\n",
    "# regr = LinearRegression()\n",
    "regr = LinearRegression()\n",
    "# regr.fit(...)\n",
    "regr.fit(Z, ytr)\n",
    "# TODO\n",
    "# Extract the parameters from regr.coef_ and regr.intercept_ and store the parameter vector in winit\n",
    "# winit = ...\n",
    "W = regr.coef_.flatten()\n",
    "b = regr.intercept_\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the predicted value `yhat` versus `x` using your initial parameter estimate `winit`.  Use 1000 values of `x` uniformly spaced over the interval `[0,1]`.  On the same plot, superimpose the points `(xts,yts)`.  You will see that the prediction curve has some singularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# xp = ...\n",
    "# yhat = ...\n",
    "# plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the prediction curve is that the denominator in our polynomial model for `yhat` goes to zero at certain values of `x` in `[0,1]`.  As a result, some of the `z` features become correlated, and the least-squares fit for `winit` includes relatively large coefficient values. To avoid this problem, we can use Ridge regression in an effort to keep the `winit` parameters closer to zero.  Re-run the fit above with `Ridge` with `alpha = 1e-2`. You should see a much better (but not perfect) fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO.  Fit with parameters with linear regression\n",
    "# regr = Ridge(alpha=1e-3)\n",
    "# regr.fit(...)\n",
    "\n",
    "# TODO\n",
    "# Extract the parameters from regr.coef_ and regr.intercept_\n",
    "# winit = ...\n",
    "\n",
    "# TODO\n",
    "# Plot the results as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Loss Function\n",
    "\n",
    "We can now use gradient descent to improve our initial estimate of the weights `w`.  Complete the construction of the following function, which is used to compute the cost\n",
    "\n",
    "    f(w) = 0.5*\\sum_i (y[i] - yhat[i])^2\n",
    "    \n",
    "and `fgrad`, the gradient of `f(w)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feval(w,x,y):\n",
    "        \n",
    "    \n",
    "    # TODO.  Parse w\n",
    "    # a = ...\n",
    "    # b = ...\n",
    "    \n",
    "    # TODO.  Znum[i,j] = x[i]**j\n",
    "\n",
    "    # TODO.  Zden[i,j] = x[i]**(j+1)\n",
    "    \n",
    "    # TODO.  Compute yhat \n",
    "    # Compute the numerator and denominator\n",
    "    \n",
    "    # TODO.  Compute loss\n",
    "    # f = ...\n",
    "    \n",
    "    # TODO.  Compute gradients\n",
    "    # fgrad = ...\n",
    "    \n",
    "    return f, fgrad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, test your gradient computation:\n",
    "* Set `w0=winit` and compute `f0,fgrad0 = feval(w0,xtr,ytr)`\n",
    "* Take a `w1` very close to `w0` and compute `f1,fgrad1 = feval(w1,xtr,ytr)`\n",
    "* Verify that `f1-f0` is close to the predicted value based on the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement gradient descent\n",
    "\n",
    "We will now try to minimize the loss function using gradient descent.  Using the function `feval` defined above, implement gradient descent.  Run gradient descent with a step size of `alpha=1e-6` starting at `w=winit`.  Run it for `nit=10000` iterations.  Compute `fgd[it]`= the objective function on iteration `it`.  Plot `fgd[it]` vs. `it`.  \n",
    "\n",
    "You should see that the training loss decreases but does not fully converge after 10000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# fgd = ...\n",
    "nit = 10000\n",
    "step = 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to get a faster convergence using adaptive step-size via the Armijo rule. Implement Armijo gradient descent.  Let `fadapt[it]` be the loss value that it attains on iteration `it`.  Plot `fadapt[it]` and `fgd[it]` vs. `it` on the same graph.  You should see some improvement, but also some lingering problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# fadapt = ...\n",
    "nit = 10000\n",
    "step = 1e-6  # Initial step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the final estimate for `w` from the adaptive step-size approach, plot the predicted value of `yhat` vs. `x` for 1000 values of `x` in `[0,1]`.  On the same plot, plot `yhat` vs. `x` for the initial parameter `w=winit`.  Also, plot the test data, `yts` vs. `xts`.  You should see that gradient descent was able to improve the prediction model, although the initial prediction model was not too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# xp = np.linspace(...)\n",
    "# yhat = ...\n",
    "# plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Gradient Descent\n",
    "\n",
    "This section is optional.\n",
    "\n",
    "One way to improve gradient descent is to use *momentum*.  With momentum, the gradient-descent update rule becomes:\n",
    "\n",
    "    f, fgrad = feval(w,...)\n",
    "    z = beta*z + fgrad\n",
    "    w = w - step*z\n",
    "    \n",
    "This is similar to gradient descent, except that the update direction `z` is the sum of the gradient `fgrad` and the previous update direction `z`, which tends to keep the algorithm moving in the same direction (instead of randomly changing directions, as it would if `z=fgrad`).  Implement momentum gradient-descent with `beta = 0.99` and `step=1e-5`.  Compare the convergence of this approach to plain gradient descent and the adaptive stepsize version.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "nit = 1000\n",
    "step = 1e-5\n",
    "beta = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# plot yhat vs. x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond This Lab\n",
    "In this lab, we have just touched at some of the ideas in optimization.  There are several other important algorithms that you can explore:\n",
    "* [Levenberg-Marquardt](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm) method for non-linear least squares\n",
    "* Newton's method\n",
    "* More difficult non-linear least squares problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
